{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02dc42c8",
   "metadata": {},
   "source": [
    "## Code to Extract Exponential Smoothing Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0982b099",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f3ecf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load modules\n",
    "import statsmodels.tsa.holtwinters as sm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d315e1ee",
   "metadata": {},
   "source": [
    "We need to read in each protected dataset, fit the SES model, and extract the $\\alpha$ parameter for each series. Save these as a dataset, and plot in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c03b2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results file path\n",
    "results_path = \"../../Outputs/Results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e22ca815",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../../Data/Train/Clean/m3_monthly_micro_h1.csv\", header=None, skiprows=1)\n",
    "# convert to a list of series, and drop missing values\n",
    "train_data = [x.dropna() for _, x in train_data.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fc3d5ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(\"../../Data/Train/Clean/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a96ca8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in files if (\"protected_m3_monthly_micro_h1_\" in f or \"full_m3\" in f)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8d09c99d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['full_m3_monthly_micro_clean.csv',\n",
       " 'protected_m3_monthly_micro_h1_AN_0.5.csv',\n",
       " 'protected_m3_monthly_micro_h1_AN_1.5.csv',\n",
       " 'protected_m3_monthly_micro_h1_AN_1.csv',\n",
       " 'protected_m3_monthly_micro_h1_AN_2.csv',\n",
       " 'protected_m3_monthly_micro_h1_Bottom_0.1.csv',\n",
       " 'protected_m3_monthly_micro_h1_Bottom_0.2.csv',\n",
       " 'protected_m3_monthly_micro_h1_Bottom_0.4.csv',\n",
       " 'protected_m3_monthly_micro_h1_DP_0.1.csv',\n",
       " 'protected_m3_monthly_micro_h1_DP_1.csv',\n",
       " 'protected_m3_monthly_micro_h1_DP_10.csv',\n",
       " 'protected_m3_monthly_micro_h1_DP_20.csv',\n",
       " 'protected_m3_monthly_micro_h1_DP_4.6.csv',\n",
       " 'protected_m3_monthly_micro_h1_k-nts_10.csv',\n",
       " 'protected_m3_monthly_micro_h1_k-nts_15.csv',\n",
       " 'protected_m3_monthly_micro_h1_k-nts_5.csv',\n",
       " 'protected_m3_monthly_micro_h1_Top_0.1.csv',\n",
       " 'protected_m3_monthly_micro_h1_Top_0.2.csv',\n",
       " 'protected_m3_monthly_micro_h1_Top_0.4.csv']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1c56c175",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = {\"full\":[], \"DP\": [0.1, 1, 4.6, 10, 20], \"Top\": [0.1, 0.2, 0.4], \"k-nts\": [5, 10, 15]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b60d3bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_dict = {}\n",
    "for method in methods.items():\n",
    "    method_files = [f for f in files if method[0] in f]\n",
    "    \n",
    "    if method[0] == \"full\":\n",
    "        param_files = method_files\n",
    "        [param_files] = param_files\n",
    "        \n",
    "        full_data = pd.read_csv(\"../../Data/Train/Clean/\"+param_files, header=None, skiprows=1)\n",
    "        full_data = [x.dropna() for _, x in full_data.iterrows()]\n",
    "        full_data = [pd.Series([i if i >= 1 else 1 for i in x]) for x in full_data]\n",
    "        full_data = [np.log(x) for x in full_data]\n",
    "        \n",
    "        alpha = []\n",
    "        for x in full_data:\n",
    "            ES = sm.ExponentialSmoothing(endog=x, initialization_method = 'estimated')\n",
    "            ses = ES.fit()\n",
    "            alpha.append(ses.params['smoothing_level'])\n",
    "            \n",
    "        alpha_dict[\"original\"] = alpha\n",
    "    \n",
    "    else: \n",
    "        for param in method[1]:\n",
    "            param_files = [f for f in method_files if \"_\"+str(param)+\".\" in f]\n",
    "            [param_files] = param_files\n",
    "            \n",
    "            full_data = pd.read_csv(\"../../Data/Train/Clean/\"+param_files, header=None, skiprows=1)\n",
    "            full_data = [x.dropna() for _, x in full_data.iterrows()]\n",
    "            full_data = [pd.Series([i if i >= 1 else 1 for i in x]) for x in full_data]\n",
    "            full_data = [np.log(x) for x in full_data]\n",
    "            \n",
    "            alpha = []\n",
    "            for x in full_data:\n",
    "                ES = sm.ExponentialSmoothing(endog=x, initialization_method = 'estimated')\n",
    "                ses = ES.fit()\n",
    "                alpha.append(ses.params['smoothing_level'])\n",
    "            \n",
    "            alpha_dict[method[0]+\"_\"+str(param)] = alpha\n",
    "    \n",
    "\n",
    "        \n",
    "#         ES = sm.ExponentialSmoothing(endog = x,                           # time series to model\n",
    "#                              initialization_method = 'estimated') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5accb748",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_frame = pd.DataFrame(alpha_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "33fa34dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_frame.to_csv(\"../../Outputs/Results/Model Parameters/ses_alphas.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f7fa76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9fc917",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c794338",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de331027",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
