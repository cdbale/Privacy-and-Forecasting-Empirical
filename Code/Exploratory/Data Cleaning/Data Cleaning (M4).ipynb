{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0512bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495abe35",
   "metadata": {},
   "source": [
    "Since our method requires *k* nearest neighbors, we will drop any series within a category that do not have the same length as *k* other series. The maximum value of *k* we consider is 15, so we will require at least 16 series of each length in each category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf8dd5e",
   "metadata": {},
   "source": [
    "## Write a function to import and split the series by length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6f9fe7",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdc274ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_split_save(time_series_frequency, min_number_series):\n",
    "    \n",
    "    '''\n",
    "    Function to import a subset of the M4 data. Filter out series that do not have at least min_number_series\n",
    "    of the same length.\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "    time_series_frequency (str): frequency corresponding to the desired file (accepts yearly, quarterly, monthly, other)\n",
    "    min_number_series (int): minimum number of series that must have the same length to be kept in the data. Allows us to\n",
    "        use global forecasting models and swap values between nearest neighbor time series. Select a value equal to (k + 1) \n",
    "        where k is the largest value you will consider for swapping.\n",
    "    '''\n",
    "    \n",
    "    # import train and test data\n",
    "    temp_train = pd.read_csv(\"../../../Data/Train/\" + time_series_frequency + \"-train.csv\").iloc[:,1:]\n",
    "    temp_test = pd.read_csv(\"../../../Data/Test/\" + time_series_frequency + \"-test.csv\").iloc[:,1:]\n",
    "    \n",
    "    # remove the missing values from the ends of series\n",
    "    temp_train = [x.dropna() for i, x in temp_train.iterrows()]\n",
    "    temp_test = [x.dropna() for i, x in temp_test.iterrows()]\n",
    "    \n",
    "    # compute the length of each training series\n",
    "    train_lengths = pd.Series([len(x) for x in temp_train])\n",
    "    \n",
    "    # count how many series have each length\n",
    "    length_counts = train_lengths.value_counts()\n",
    "\n",
    "    # indicate which lengths to keep based on which have at least min_number_series series\n",
    "    length_keep = length_counts.index[length_counts >= min_number_series]\n",
    "    \n",
    "    # logical vector - which series are we keeping based on length\n",
    "    to_keep = train_lengths.isin(length_keep)\n",
    "    \n",
    "    # restrict train and test data to the series with the appropriate lengths\n",
    "    temp_train = [x for i,x in enumerate(temp_train) if to_keep.iloc[i]]\n",
    "    temp_test = [x for i,x in enumerate(temp_test) if to_keep.iloc[i]]\n",
    "    \n",
    "    # now, sort based on length so identical length series are grouped together\n",
    "    sort_ids = np.argsort([len(x) for x in temp_train])\n",
    "    temp_train = [temp_train[x] for x in sort_ids]\n",
    "    temp_test = [temp_test[x] for x in sort_ids]\n",
    "    \n",
    "    # used for machine learning feature selection for k-nTS+\n",
    "    train_2, test_2 = pd.DataFrame([x.iloc[:-1] for x in temp_train]), pd.DataFrame([x.iloc[-1] for x in temp_train])\n",
    "        \n",
    "    # used for assessing final forecast accuracy\n",
    "    train_1 = pd.DataFrame(temp_train)\n",
    "    test_1 = pd.DataFrame([x.iloc[0] for x in temp_test])\n",
    "    \n",
    "    # now save all data sets\n",
    "    train_2.to_csv(\"../../../Data/Cleaned/M4/\" + time_series_frequency + \"_h2_train\" + \".csv\", index=False)\n",
    "    train_1.to_csv(\"../../../Data/Cleaned/M4/\" + time_series_frequency + \"_h1_train\" + \".csv\", index=False)\n",
    "    test_2.to_csv(\"../../../Data/Cleaned/M4/\" + time_series_frequency + \"_h2_test\" + \".csv\", index=False)\n",
    "    test_1.to_csv(\"../../../Data/Cleaned/M4/\" + time_series_frequency + \"_h1_test\" + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14992e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[import_split_save(freq, 16) for freq in [\"Yearly\", \"Quarterly\", \"Monthly\"]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
